---
title: "Linear Regression  and Non Linear Regression"
author: "Zeel"
date: "2023-11-04"
categories: [Machine Learning]
---


#Linear and Non-Linear Regression

Linear regression helps you understand and quantify the relationship between two variables, making it easier to make predictions or draw conclusions about one variable based on the other

##Method 1:

```{python}
# Import Libary
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Download and prepare the data
df=pd.read_csv("headbrain.csv")

df.head()

```



```{python}
# Import Libary
print(df.isnull().sum())

	
# Declare dependent variable(Y) and independent variable(X)

X=df['Head Size(cm^3)'].values
Y = df['Brain Weight(grams)'].values

```

```{python}
df.info()

```

```{python}

df.shape
```

```{python}
X.shape
```


```{python}

Y.shape

```

```{python}
np.corrcoef(X, Y)

```


```{python}
# Plot the Input Data
plt.scatter(X, Y, c='green', label='Data points')
plt.xlabel('Head Size in cm3')
plt.ylabel('Brain Weight in grams')
plt.legend()
plt.show()
```



```{python}
# Plot the Input Data
# Calculating coefficient

# Mean X and Y
mean_x = np.mean(X)
mean_y = np.mean(Y)

# Total number of values
n = len(X)

# Using the formula to calculate theta1 and theta2
numer = 0
denom = 0
for i in range(n):
    numer += (X[i] - mean_x) * (Y[i] - mean_y)
    denom += (X[i] - mean_x) ** 2
b1 = numer / denom
b0 = mean_y - (b1 * mean_x)

# Printing coefficients
print("coefficients for regression",b1, b0)
```

```{python}
# Plotting Values and Regression Line
%matplotlib inline

plt.rcParams['figure.figsize'] = (10.0, 5.0)
# max_x = np.max(X) + 100
# min_x = np.min(X) - 100

y = b0 + b1 * X

# Ploting Line
plt.plot(X, y, color='blue', label='Regression Line')
# Ploting Scatter Points
plt.scatter(X, Y, c='green', label='Scatter data')

plt.xlabel('Head Size in cm3')
plt.ylabel('Brain Weight in grams')
plt.legend()
plt.show()
```

```{python}
# Calculating Root Mean Squares Error
rmse = 0
for i in range(n):
    y_pred = b0 + b1 * X[i]
    rmse += (Y[i] - y_pred) ** 2
    
rmse = np.sqrt(rmse/n)
print("Root Mean Square Error is",rmse)
```

```{python}
# Calculating R2 Score
ss_tot = 0
ss_res = 0
for i in range(n):
    y_pred = b0 + b1 * X[i]
    ss_tot += (Y[i] - mean_y) ** 2
    ss_res += (Y[i] - y_pred) ** 2
r2 = 1 - (ss_res/ss_tot)
print("R2 Score",r2)
```


##Method 2:


```{python}
# Import necessary libraries
import pandas as pd

# Load the dataset
df = pd.read_csv('headbrain.csv')

# Explore the dataset
df.head()
```

```{python}
print(df.isnull().sum())

```



```{python}
mean_x = np.mean(X)
mean_y = np.mean(Y)

#Total number of Values
n = len(X)

```

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

X = X.reshape((n,1))

```

```{python}
model = LinearRegression()
```

```{python}
model = model.fit(X,Y)
```

```{python}
r2 = model.score(X,Y)
print('R^2 value: ',r2)
```


```{python}
from sklearn.model_selection import train_test_split

X = df[['Head Size(cm^3)']]  # Select relevant features
y = df['Brain Weight(grams)']  # Define the target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

```{python}
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

model = LinearRegression() # Create a linear regression model

model.fit(X_train, y_train) # Fit the model to the training data

```

```{python}
y_pred = model.predict(X_test)
```

```{python}

plt.scatter(y_test, y_pred) # Scatter plot to visualize actual vs. predicted values

plt.xlabel("Actual Sale Prices")
plt.ylabel("Predicted Sale Prices")
plt.title("Actual Sale Prices vs. Predicted Sale Prices")
plt.show()
```

```{python}
# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate R-squared (R²) score
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R²) Score: {r2:.2f}")
```





#Non Linear Regression

Non-linear regression allows for more flexibility in modeling relationships that don't follow a straight-line pattern. It's useful when the data suggests a more complex connection between variables

##Method 1 (Manual dataset)

```{python}
import matplotlib.pyplot as plt
import numpy as np

#define predictor and response variables
x = np.array([2, 3, 4, 5, 6, 7, 7, 8, 9, 11, 12])
y = np.array([18, 16, 15, 17, 20, 23, 25, 28, 31, 30, 29])

#create scatterplot to visualize relationship between x and y
plt.scatter(x, y)
```

```{python}
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

#specify degree of 3 for polynomial regression model
#include bias=False means don't force y-intercept to equal zero
poly = PolynomialFeatures(degree=3, include_bias=False)

#reshape data to work properly with sklearn
poly_features = poly.fit_transform(x.reshape(-1, 1))

#fit polynomial regression model
poly_reg_model = LinearRegression()
poly_reg_model.fit(poly_features, y)

#display model coefficients
print(poly_reg_model.intercept_, poly_reg_model.coef_)
```

```{python}
#use model to make predictions on response variable
y_predicted = poly_reg_model.predict(poly_features)

#create scatterplot of x vs. y
plt.scatter(x, y)

#add line to show fitted polynomial regression model
plt.plot(x, y_predicted, color='purple')
```


##Method 2

```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
#from sklearn.linear_model import LinearRegression

# Download and prepare the data
df=pd.read_csv("headbrain.csv")

df.head()
```

```{python}

X=df['Head Size(cm^3)'].values
Y = df['Brain Weight(grams)'].values
```

```{python}


# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
X=X.reshape(-1,1)
Y=Y.reshape(-1,1)
poly = PolynomialFeatures(degree=4)
X_poly = poly.fit_transform(X)
 
poly.fit(X_poly, Y)
lin2 = LinearRegression()


lin2.fit(X_poly, Y)
```

```{python}



# Visualising the Polynomial Regression results
plt.scatter(X, Y, color='blue')
 
plt.plot(X, lin2.predict(poly.fit_transform(X)),
         color='red')
plt.title('Polynomial Regression')
plt.xlabel('Head')
plt.ylabel('Brain')
 
plt.show()
```

#Conclusion

The code for Linear Regression explores a dataset related to head size and brain weight.
It performs simple linear regression, calculates coefficients, and visualizes the regression line.
The Root Mean Squared Error (RMSE) is computed as a measure of model accuracy.
The R2 score is calculated to assess the goodness of fit of the regression model.
In summary, the code aims to understand and model the linear relationship between head size and brain weight, providing insights into the predictive capability of head size for brain weight.


The code for non Linear Regression loads a dataset related to head size and brain weight.
Polynomial regression of degree 4 is applied to model the relationship between head size and brain weight.
The polynomial regression model is fitted to the data.
