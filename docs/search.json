[
  {
    "objectID": "posts/blog5/index.html",
    "href": "posts/blog5/index.html",
    "title": "Blog1",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom numpy import percentile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.ensemble import IsolationForest\n\nfrom scipy import stats\n\n\ndf = pd.read_csv('Superstore.csv')\ndf.head()\n\n\n\n\n\n\n\n\nRow ID\nOrder ID\nOrder Date\nShip Date\nShip Mode\nCustomer ID\nCustomer Name\nSegment\nCountry\nCity\n...\nPostal Code\nRegion\nProduct ID\nCategory\nSub-Category\nProduct Name\nSales\nQuantity\nDiscount\nProfit\n\n\n\n\n0\n1\nCA-2016-152156\n08-11-2016\n11-11-2016\nSecond Class\nCG-12520\nClaire Gute\nConsumer\nUnited States\nHenderson\n...\n42420\nSouth\nFUR-BO-10001798\nFurniture\nBookcases\nBush Somerset Collection Bookcase\n261.9600\n2\n0.00\n41.9136\n\n\n1\n2\nCA-2016-152156\n08-11-2016\n11-11-2016\nSecond Class\nCG-12520\nClaire Gute\nConsumer\nUnited States\nHenderson\n...\n42420\nSouth\nFUR-CH-10000454\nFurniture\nChairs\nHon Deluxe Fabric Upholstered Stacking Chairs,...\n731.9400\n3\n0.00\n219.5820\n\n\n2\n3\nCA-2016-138688\n12-06-2016\n16-06-2016\nSecond Class\nDV-13045\nDarrin Van Huff\nCorporate\nUnited States\nLos Angeles\n...\n90036\nWest\nOFF-LA-10000240\nOffice Supplies\nLabels\nSelf-Adhesive Address Labels for Typewriters b...\n14.6200\n2\n0.00\n6.8714\n\n\n3\n4\nUS-2015-108966\n11-10-2015\n18-10-2015\nStandard Class\nSO-20335\nSean O'Donnell\nConsumer\nUnited States\nFort Lauderdale\n...\n33311\nSouth\nFUR-TA-10000577\nFurniture\nTables\nBretford CR4500 Series Slim Rectangular Table\n957.5775\n5\n0.45\n-383.0310\n\n\n4\n5\nUS-2015-108966\n11-10-2015\n18-10-2015\nStandard Class\nSO-20335\nSean O'Donnell\nConsumer\nUnited States\nFort Lauderdale\n...\n33311\nSouth\nOFF-ST-10000760\nOffice Supplies\nStorage\nEldon Fold 'N Roll Cart System\n22.3680\n2\n0.20\n2.5164\n\n\n\n\n5 rows × 21 columns\n\n\n\n\nplt.scatter(range(df.shape[0]), np.sort(df['Sales'].values))\nplt.xlabel('index')\nplt.ylabel('Sales')\nplt.title(\"Sales distribution\")\nsns.despine()\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.distplot(df['Sales'])\nplt.title(\"Distribution of Sales\")\nsns.despine()\n\n\n\n\n\nprint(\"Skewness: %f\" % df['Sales'].skew())\nprint(\"Kurtosis: %f\" % df['Sales'].kurt())\n\nSkewness: 12.972752\nKurtosis: 305.311753\n\n\n\ndf.Profit.describe()\n\ncount    9994.000000\nmean       28.656896\nstd       234.260108\nmin     -6599.978000\n25%         1.728750\n50%         8.666500\n75%        29.364000\nmax      8399.976000\nName: Profit, dtype: float64\n\n\n\nplt.scatter(range(df.shape[0]), np.sort(df['Profit'].values))\nplt.xlabel('index')\nplt.ylabel('Profit')\nplt.title(\"Profit distribution\")\nsns.despine()\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.distplot(df['Profit'])\nplt.title(\"Distribution of Profit\")\nsns.despine()\n\n\n\n\n\nprint(\"Skewness: %f\" % df['Profit'].skew())\nprint(\"Kurtosis: %f\" % df['Profit'].kurt())\n\nSkewness: 7.561432\nKurtosis: 397.188515\n\n\nUnivariate Anomaly Detection on Sales\n\nisolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\nisolation_forest.fit(df['Sales'].values.reshape(-1, 1))\ndf['anomaly_score_univariate_sales'] = isolation_forest.decision_function(df['Sales'].values.reshape(-1, 1))\ndf['outlier_univariate_sales'] = isolation_forest.predict(df['Sales'].values.reshape(-1, 1))\n\n\nxx = np.linspace(df['Sales'].min(), df['Sales'].max(), len(df)).reshape(-1,1)\nanomaly_score = isolation_forest.decision_function(xx)\noutlier = isolation_forest.predict(xx)\n\nplt.figure(figsize=(10,4))\nplt.plot(xx, anomaly_score, label='anomaly score')\nplt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n                 where=outlier==-1, color='r', \n                 alpha=.4, label='outlier region')\nplt.legend()\nplt.ylabel('anomaly score')\nplt.xlabel('Sales')\nplt.show();\n\n\n\n\nAnomaly detection on Profit\n\nisolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\nisolation_forest.fit(df['Profit'].values.reshape(-1, 1))\ndf['anomaly_score_univariate_profit'] = isolation_forest.decision_function(df['Profit'].values.reshape(-1, 1))\ndf['outlier_univariate_profit'] = isolation_forest.predict(df['Profit'].values.reshape(-1, 1))\n\n\ndf.sort_values('anomaly_score_univariate_profit')\n\n\n\n\n\n\n\n\nRow ID\nOrder ID\nOrder Date\nShip Date\nShip Mode\nCustomer ID\nCustomer Name\nSegment\nCountry\nCity\n...\nSub-Category\nProduct Name\nSales\nQuantity\nDiscount\nProfit\nanomaly_score_univariate_sales\noutlier_univariate_sales\nanomaly_score_univariate_profit\noutlier_univariate_profit\n\n\n\n\n8153\n8154\nCA-2017-140151\n23-03-2017\n25-03-2017\nFirst Class\nRB-19360\nRaymond Buch\nConsumer\nUnited States\nSeattle\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n13999.960\n4\n0.0\n6719.9808\n-0.114327\n-1\n-0.097654\n-1\n\n\n6826\n6827\nCA-2016-118689\n02-10-2016\n09-10-2016\nStandard Class\nTC-20980\nTamara Chand\nCorporate\nUnited States\nLafayette\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n17499.950\n5\n0.0\n8399.9760\n-0.115502\n-1\n-0.097654\n-1\n\n\n4190\n4191\nCA-2017-166709\n17-11-2017\n22-11-2017\nStandard Class\nHL-15040\nHunter Lopez\nConsumer\nUnited States\nNewark\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n10499.970\n3\n0.0\n5039.9856\n-0.110812\n-1\n-0.093651\n-1\n\n\n9039\n9040\nCA-2016-117121\n17-12-2016\n21-12-2016\nStandard Class\nAB-10105\nAdrian Barton\nConsumer\nUnited States\nDetroit\n...\nBinders\nGBC Ibimaster 500 Manual ProClick Binding System\n9892.740\n13\n0.0\n4946.3700\n-0.107894\n-1\n-0.093651\n-1\n\n\n4098\n4099\nCA-2014-116904\n23-09-2014\n28-09-2014\nStandard Class\nSC-20095\nSanjit Chand\nConsumer\nUnited States\nMinneapolis\n...\nBinders\nIbico EPK-21 Electric Binding System\n9449.950\n5\n0.0\n4630.4755\n-0.107894\n-1\n-0.092511\n-1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5183\n5184\nCA-2017-163335\n24-09-2017\n27-09-2017\nFirst Class\nAG-10675\nAnna Gayman\nConsumer\nUnited States\nColumbus\n...\nStorage\nFellowes Desktop Hanging File Manager\n40.290\n3\n0.0\n10.0725\n0.348982\n1\n0.426514\n1\n\n\n4838\n4839\nCA-2017-106831\n01-06-2017\n03-06-2017\nFirst Class\nFH-14350\nFred Harton\nConsumer\nUnited States\nDublin\n...\nPaper\nXerox 1924\n27.744\n6\n0.2\n10.0572\n0.357825\n1\n0.426514\n1\n\n\n6066\n6067\nCA-2015-151547\n17-01-2015\n23-01-2015\nStandard Class\nAH-10465\nAmy Hunt\nConsumer\nUnited States\nBartlett\n...\nSupplies\nAcme Hot Forged Carbon Steel Scissors with Nic...\n88.960\n8\n0.2\n10.0080\n0.320779\n1\n0.426514\n1\n\n\n9309\n9310\nCA-2014-128237\n25-03-2014\n30-03-2014\nStandard Class\nCA-12265\nChristina Anderson\nConsumer\nUnited States\nSan Francisco\n...\nArt\nBIC Brite Liner Grip Highlighters, Assorted, 5...\n25.440\n6\n0.0\n9.9216\n0.359720\n1\n0.426514\n1\n\n\n6895\n6896\nCA-2017-164756\n18-09-2017\n22-09-2017\nStandard Class\nSS-20140\nSaphhira Shifley\nCorporate\nUnited States\nColumbus\n...\nSupplies\nFiskars 8\" Scissors, 2/Pack\n34.480\n2\n0.0\n9.9992\n0.355954\n1\n0.426514\n1\n\n\n\n\n9994 rows × 25 columns\n\n\n\n\nxx = np.linspace(df['Profit'].min(), df['Profit'].max(), len(df)).reshape(-1,1)\nanomaly_score = isolation_forest.decision_function(xx)\noutlier = isolation_forest.predict(xx)\nplt.figure(figsize=(10,4))\nplt.plot(xx, anomaly_score, label='anomaly score')\nplt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n                 where=outlier==-1, color='r', \n                 alpha=.4, label='outlier region')\nplt.legend()\nplt.ylabel('anomaly score')\nplt.xlabel('Profit')\nplt.show();\n\n\n\n\n\ndf[(df['outlier_univariate_profit']==-1) & (df['outlier_univariate_sales']==-1)]\n\n\n\n\n\n\n\n\nRow ID\nOrder ID\nOrder Date\nShip Date\nShip Mode\nCustomer ID\nCustomer Name\nSegment\nCountry\nCity\n...\nSub-Category\nProduct Name\nSales\nQuantity\nDiscount\nProfit\nanomaly_score_univariate_sales\noutlier_univariate_sales\nanomaly_score_univariate_profit\noutlier_univariate_profit\n\n\n\n\n27\n28\nUS-2015-150630\n17-09-2015\n21-09-2015\nStandard Class\nTB-21520\nTracy Blumstein\nConsumer\nUnited States\nPhiladelphia\n...\nBookcases\nRiverside Palais Royal Lawyers Bookcase, Royal...\n3083.430\n7\n0.5\n-1665.0522\n-0.023737\n-1\n-0.061750\n-1\n\n\n165\n166\nCA-2014-139892\n08-09-2014\n12-09-2014\nStandard Class\nBM-11140\nBecky Martin\nConsumer\nUnited States\nSan Antonio\n...\nMachines\nLexmark MX611dhe Monochrome Laser Printer\n8159.952\n8\n0.4\n-1359.9920\n-0.104405\n-1\n-0.057917\n-1\n\n\n318\n319\nCA-2014-164973\n04-11-2014\n09-11-2014\nStandard Class\nNM-18445\nNathan Mautz\nHome Office\nUnited States\nNew York City\n...\nMachines\nCanon imageCLASS MF7460 Monochrome Digital Las...\n3991.980\n2\n0.0\n1995.9900\n-0.061972\n-1\n-0.071136\n-1\n\n\n353\n354\nCA-2016-129714\n01-09-2016\n03-09-2016\nFirst Class\nAB-10060\nAdam Bellavance\nHome Office\nUnited States\nNew York City\n...\nBinders\nGBC DocuBind P400 Electric Binding System\n4355.168\n4\n0.2\n1415.4296\n-0.074197\n-1\n-0.056825\n-1\n\n\n509\n510\nCA-2015-145352\n16-03-2015\n22-03-2015\nStandard Class\nCM-12385\nChristopher Martinez\nConsumer\nUnited States\nAtlanta\n...\nBinders\nFellowes PB500 Electric Punch Plastic Comb Bin...\n6354.950\n5\n0.0\n3177.4750\n-0.094598\n-1\n-0.081755\n-1\n\n\n515\n516\nCA-2017-127432\n22-01-2017\n27-01-2017\nStandard Class\nAD-10180\nAlan Dominguez\nHome Office\nUnited States\nGreat Falls\n...\nCopiers\nCanon Image Class D660 Copier\n2999.950\n5\n0.0\n1379.9770\n-0.019543\n-1\n-0.056279\n-1\n\n\n683\n684\nUS-2017-168116\n04-11-2017\n04-11-2017\nSame Day\nGT-14635\nGrant Thornton\nCorporate\nUnited States\nBurlington\n...\nMachines\nCubify CubeX 3D Printer Triple Head Print\n7999.980\n4\n0.5\n-3839.9904\n-0.103246\n-1\n-0.078947\n-1\n\n\n994\n995\nCA-2014-117639\n21-05-2014\n25-05-2014\nStandard Class\nMW-18235\nMitch Willingham\nCorporate\nUnited States\nVirginia Beach\n...\nBinders\nFellowes PB300 Plastic Comb Binding Machine\n2715.930\n7\n0.0\n1276.4871\n-0.014852\n-1\n-0.055189\n-1\n\n\n1085\n1086\nUS-2016-143819\n01-03-2016\n05-03-2016\nStandard Class\nKD-16270\nKaren Daniels\nConsumer\nUnited States\nYonkers\n...\nMachines\nAtiva V4110MDD Micro-Cut Shredder\n4899.930\n7\n0.0\n2400.9657\n-0.083202\n-1\n-0.081192\n-1\n\n\n1454\n1455\nCA-2016-133711\n26-11-2016\n29-11-2016\nFirst Class\nMC-17425\nMark Cousins\nCorporate\nUnited States\nMobile\n...\nMachines\nHewlett-Packard Deskjet 3050a All-in-One Color...\n3040.000\n8\n0.0\n1459.2000\n-0.022162\n-1\n-0.056825\n-1\n\n\n1644\n1645\nCA-2015-111829\n19-03-2015\n20-03-2015\nFirst Class\nFH-14365\nFred Hopkins\nCorporate\nUnited States\nSeattle\n...\nCopiers\nCanon PC940 Copier\n3149.930\n7\n0.0\n1480.4671\n-0.025843\n-1\n-0.057917\n-1\n\n\n1803\n1804\nCA-2017-158379\n22-09-2017\n26-09-2017\nSecond Class\nJA-15970\nJoseph Airdo\nConsumer\nUnited States\nPhiladelphia\n...\nSupplies\nMartin Yale Chadless Opener Electric Letter Op...\n4663.736\n7\n0.2\n-1049.3406\n-0.078687\n-1\n-0.047602\n-1\n\n\n2182\n2183\nCA-2016-128818\n07-05-2016\n11-05-2016\nStandard Class\nCJ-12010\nCaroline Jumper\nConsumer\nUnited States\nNew York City\n...\nMachines\nBady BDG101FRU Card Printer\n3999.950\n5\n0.0\n1159.9855\n-0.061972\n-1\n-0.049222\n-1\n\n\n2492\n2493\nCA-2014-144624\n19-11-2014\n23-11-2014\nStandard Class\nJM-15865\nJohn Murray\nConsumer\nUnited States\nJamestown\n...\nPhones\nApple iPhone 5\n4548.810\n7\n0.0\n1228.1787\n-0.078125\n-1\n-0.052471\n-1\n\n\n2623\n2624\nCA-2017-127180\n22-10-2017\n24-10-2017\nFirst Class\nTA-21385\nTom Ashbrook\nHome Office\nUnited States\nNew York City\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n11199.968\n4\n0.2\n3919.9888\n-0.111397\n-1\n-0.086832\n-1\n\n\n2697\n2698\nCA-2014-145317\n18-03-2014\n23-03-2014\nStandard Class\nSM-20320\nSean Miller\nHome Office\nUnited States\nJacksonville\n...\nMachines\nCisco TelePresence System EX90 Videoconferenci...\n22638.480\n6\n0.5\n-1811.0784\n-0.115502\n-1\n-0.065050\n-1\n\n\n2848\n2849\nCA-2017-157854\n08-04-2017\n15-04-2017\nStandard Class\nDM-13345\nDenise Monton\nCorporate\nUnited States\nRoswell\n...\nBinders\nGBC DocuBind TL300 Electric Binding System\n2690.970\n3\n0.0\n1264.7559\n-0.012776\n-1\n-0.055189\n-1\n\n\n3011\n3012\nCA-2017-134845\n17-04-2017\n23-04-2017\nStandard Class\nSR-20425\nSharelle Roach\nHome Office\nUnited States\nLouisville\n...\nMachines\nLexmark MX611dhe Monochrome Laser Printer\n2549.985\n5\n0.7\n-3399.9800\n-0.007097\n-1\n-0.077267\n-1\n\n\n3055\n3056\nUS-2015-100377\n28-08-2015\n01-09-2015\nStandard Class\nTS-21370\nTodd Sumrall\nCorporate\nUnited States\nChicago\n...\nCopiers\nCanon Imageclass D680 Copier / Fax\n2799.960\n5\n0.2\n874.9875\n-0.015892\n-1\n-0.010726\n-1\n\n\n3273\n3274\nCA-2017-133865\n08-05-2017\n12-05-2017\nStandard Class\nPS-19045\nPenelope Sewall\nHome Office\nUnited States\nLos Angeles\n...\nCopiers\nCanon Imageclass D680 Copier / Fax\n3359.952\n6\n0.2\n1049.9850\n-0.034323\n-1\n-0.037420\n-1\n\n\n3983\n3984\nCA-2016-135265\n07-07-2016\n09-07-2016\nSecond Class\nCC-12370\nChristopher Conant\nConsumer\nUnited States\nLos Angeles\n...\nCopiers\nCanon PC1060 Personal Laser Copier\n2799.960\n5\n0.2\n944.9865\n-0.015892\n-1\n-0.026842\n-1\n\n\n4093\n4094\nCA-2015-102491\n24-08-2015\n28-08-2015\nStandard Class\nKW-16435\nKatrina Willman\nConsumer\nUnited States\nFlorence\n...\nMachines\nCisco 9971 IP Video Phone Charcoal\n3080.000\n7\n0.0\n1416.8000\n-0.023737\n-1\n-0.056825\n-1\n\n\n4098\n4099\nCA-2014-116904\n23-09-2014\n28-09-2014\nStandard Class\nSC-20095\nSanjit Chand\nConsumer\nUnited States\nMinneapolis\n...\nBinders\nIbico EPK-21 Electric Binding System\n9449.950\n5\n0.0\n4630.4755\n-0.107894\n-1\n-0.092511\n-1\n\n\n4128\n4129\nCA-2014-127299\n19-09-2014\n24-09-2014\nStandard Class\nJL-15835\nJohn Lee\nConsumer\nUnited States\nCharlotte\n...\nMachines\nHP Designjet T520 Inkjet Large Format Printer ...\n2624.985\n3\n0.5\n-944.9946\n-0.009157\n-1\n-0.044910\n-1\n\n\n4190\n4191\nCA-2017-166709\n17-11-2017\n22-11-2017\nStandard Class\nHL-15040\nHunter Lopez\nConsumer\nUnited States\nNewark\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n10499.970\n3\n0.0\n5039.9856\n-0.110812\n-1\n-0.093651\n-1\n\n\n4277\n4278\nUS-2016-107440\n16-04-2016\n20-04-2016\nStandard Class\nBS-11365\nBill Shonely\nCorporate\nUnited States\nLakewood\n...\nMachines\n3D Systems Cube Printer, 2nd Generation, Magenta\n9099.930\n7\n0.0\n2365.9818\n-0.106148\n-1\n-0.081192\n-1\n\n\n4619\n4620\nCA-2017-145219\n24-12-2017\n25-12-2017\nFirst Class\nRM-19675\nRobert Marley\nHome Office\nUnited States\nLos Angeles\n...\nCopiers\nHewlett Packard LaserJet 3310 Copier\n2879.952\n6\n0.2\n1007.9832\n-0.016934\n-1\n-0.033171\n-1\n\n\n5126\n5127\nCA-2014-160766\n14-09-2014\n14-09-2014\nSame Day\nDM-13015\nDarrin Martin\nConsumer\nUnited States\nNew York City\n...\nMachines\nAtiva V4110MDD Micro-Cut Shredder\n2799.960\n4\n0.0\n1371.9804\n-0.015892\n-1\n-0.056279\n-1\n\n\n5198\n5199\nCA-2016-103982\n03-03-2016\n08-03-2016\nStandard Class\nAA-10315\nAlex Avila\nConsumer\nUnited States\nRound Rock\n...\nSupplies\nHigh Speed Automatic Electric Letter Opener\n3930.072\n3\n0.2\n-786.0144\n-0.057023\n-1\n-0.028946\n-1\n\n\n5562\n5563\nCA-2017-133263\n31-03-2017\n02-04-2017\nSecond Class\nJE-15610\nJim Epp\nCorporate\nUnited States\nAtlanta\n...\nCopiers\nHewlett Packard LaserJet 3310 Copier\n2999.950\n5\n0.0\n1439.9760\n-0.019543\n-1\n-0.056825\n-1\n\n\n5710\n5711\nUS-2015-160857\n08-05-2015\n15-05-2015\nStandard Class\nNW-18400\nNatalie Webber\nConsumer\nUnited States\nNew York City\n...\nCopiers\nSharp AL-1530CS Digital Copier\n2799.944\n7\n0.2\n1014.9797\n-0.015892\n-1\n-0.033171\n-1\n\n\n6209\n6210\nCA-2017-119809\n18-08-2017\n25-08-2017\nStandard Class\nYS-21880\nYana Sorensen\nCorporate\nUnited States\nSeattle\n...\nBinders\nFellowes PB300 Plastic Comb Binding Machine\n2793.528\n9\n0.2\n942.8157\n-0.015892\n-1\n-0.026317\n-1\n\n\n6340\n6341\nCA-2017-143112\n05-10-2017\n09-10-2017\nStandard Class\nTS-21370\nTodd Sumrall\nCorporate\nUnited States\nNew York City\n...\nMachines\n3D Systems Cube Printer, 2nd Generation, Magenta\n5199.960\n4\n0.0\n1351.9896\n-0.090021\n-1\n-0.056279\n-1\n\n\n6425\n6426\nCA-2016-143714\n23-05-2016\n27-05-2016\nStandard Class\nCC-12370\nChristopher Conant\nConsumer\nUnited States\nPhiladelphia\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n8399.976\n4\n0.4\n1119.9968\n-0.104405\n-1\n-0.045448\n-1\n\n\n6520\n6521\nCA-2017-138289\n16-01-2017\n18-01-2017\nSecond Class\nAR-10540\nAndy Reiter\nConsumer\nUnited States\nJackson\n...\nBinders\nGBC DocuBind P400 Electric Binding System\n5443.960\n4\n0.0\n2504.2216\n-0.090021\n-1\n-0.081192\n-1\n\n\n6626\n6627\nCA-2014-145541\n14-12-2014\n21-12-2014\nStandard Class\nTB-21400\nTom Boeckenhauer\nConsumer\nUnited States\nNew York City\n...\nMachines\nHP Designjet T520 Inkjet Large Format Printer ...\n6999.960\n4\n0.0\n2239.9872\n-0.098622\n-1\n-0.080630\n-1\n\n\n6817\n6818\nCA-2014-144414\n17-06-2014\n21-06-2014\nStandard Class\nGH-14425\nGary Hwang\nConsumer\nUnited States\nSeattle\n...\nBinders\nGBC DocuBind P400 Electric Binding System\n3266.376\n3\n0.2\n1061.5722\n-0.031663\n-1\n-0.040087\n-1\n\n\n6826\n6827\nCA-2016-118689\n02-10-2016\n09-10-2016\nStandard Class\nTC-20980\nTamara Chand\nCorporate\nUnited States\nLafayette\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n17499.950\n5\n0.0\n8399.9760\n-0.115502\n-1\n-0.097654\n-1\n\n\n7243\n7244\nCA-2017-118892\n17-08-2017\n22-08-2017\nSecond Class\nTP-21415\nTom Prescott\nConsumer\nUnited States\nPhiladelphia\n...\nChairs\nHON 5400 Series Task Chairs for Big and Tall\n4416.174\n9\n0.3\n-630.8820\n-0.074757\n-1\n-0.007646\n-1\n\n\n7280\n7281\nCA-2015-162782\n21-02-2015\n27-02-2015\nStandard Class\nPW-19240\nPierre Wener\nConsumer\nUnited States\nColumbia\n...\nBinders\nFellowes PB500 Electric Punch Plastic Comb Bin...\n2541.980\n2\n0.0\n1270.9900\n-0.003504\n-1\n-0.055189\n-1\n\n\n7583\n7584\nCA-2014-163223\n21-03-2014\n25-03-2014\nStandard Class\nKH-16690\nKristen Hastings\nCorporate\nUnited States\nSpringfield\n...\nPhones\nSamsung Galaxy S4 Active\n3499.930\n7\n0.0\n909.9818\n-0.040743\n-1\n-0.020043\n-1\n\n\n7666\n7667\nUS-2016-140158\n04-10-2016\n08-10-2016\nStandard Class\nDR-12940\nDaniel Raglin\nHome Office\nUnited States\nProvidence\n...\nCopiers\nHewlett Packard LaserJet 3310 Copier\n5399.910\n9\n0.0\n2591.9568\n-0.090021\n-1\n-0.081192\n-1\n\n\n7683\n7684\nCA-2015-120782\n28-04-2015\n01-05-2015\nFirst Class\nSD-20485\nShirley Daniels\nHome Office\nUnited States\nMidland\n...\nBinders\nFellowes PB500 Electric Punch Plastic Comb Bin...\n3812.970\n3\n0.0\n1906.4850\n-0.049928\n-1\n-0.066153\n-1\n\n\n7772\n7773\nCA-2016-108196\n25-11-2016\n02-12-2016\nStandard Class\nCS-12505\nCindy Stewart\nConsumer\nUnited States\nLancaster\n...\nMachines\nCubify CubeX 3D Printer Double Head Print\n4499.985\n5\n0.7\n-6599.9780\n-0.077000\n-1\n-0.084008\n-1\n\n\n7818\n7819\nCA-2016-138478\n21-10-2016\n26-10-2016\nSecond Class\nDP-13390\nDennis Pardue\nHome Office\nUnited States\nNorth Las Vegas\n...\nBinders\nIbico EPK-21 Electric Binding System\n4535.976\n3\n0.2\n1644.2913\n-0.077000\n-1\n-0.063949\n-1\n\n\n7914\n7915\nCA-2017-165323\n17-06-2017\n21-06-2017\nStandard Class\nSR-20740\nSteven Roelle\nHome Office\nUnited States\nNew York City\n...\nMachines\nHewlett-Packard Desktjet 6988DT Refurbished Pr...\n3404.500\n5\n0.0\n1668.2050\n-0.035923\n-1\n-0.063949\n-1\n\n\n8153\n8154\nCA-2017-140151\n23-03-2017\n25-03-2017\nFirst Class\nRB-19360\nRaymond Buch\nConsumer\nUnited States\nSeattle\n...\nCopiers\nCanon imageCLASS 2200 Advanced Copier\n13999.960\n4\n0.0\n6719.9808\n-0.114327\n-1\n-0.097654\n-1\n\n\n8204\n8205\nCA-2015-114811\n08-11-2015\n08-11-2015\nSame Day\nKD-16495\nKeith Dawkins\nCorporate\nUnited States\nNew York City\n...\nMachines\nZebra ZM400 Thermal Label Printer\n4643.800\n4\n0.0\n2229.0240\n-0.078687\n-1\n-0.080630\n-1\n\n\n8488\n8489\nCA-2016-158841\n02-02-2016\n04-02-2016\nSecond Class\nSE-20110\nSanjit Engle\nConsumer\nUnited States\nArlington\n...\nMachines\nHP Designjet T520 Inkjet Large Format Printer ...\n8749.950\n5\n0.0\n2799.9840\n-0.106148\n-1\n-0.081192\n-1\n\n\n8749\n8750\nUS-2015-163825\n16-06-2015\n19-06-2015\nFirst Class\nLC-16885\nLena Creighton\nConsumer\nUnited States\nNew York City\n...\nBinders\nFellowes PB500 Electric Punch Plastic Comb Bin...\n3050.376\n3\n0.2\n1143.8910\n-0.023737\n-1\n-0.048142\n-1\n\n\n8858\n8859\nCA-2017-135909\n13-10-2017\n20-10-2017\nStandard Class\nJW-15220\nJane Waco\nCorporate\nUnited States\nSacramento\n...\nBinders\nFellowes PB500 Electric Punch Plastic Comb Bin...\n5083.960\n5\n0.2\n1906.4850\n-0.087173\n-1\n-0.066153\n-1\n\n\n8990\n8991\nUS-2015-128587\n24-12-2015\n30-12-2015\nStandard Class\nHM-14860\nHarry Marie\nCorporate\nUnited States\nSpringfield\n...\nCopiers\nCanon PC1060 Personal Laser Copier\n4899.930\n7\n0.0\n2302.9671\n-0.083202\n-1\n-0.081192\n-1\n\n\n9039\n9040\nCA-2016-117121\n17-12-2016\n21-12-2016\nStandard Class\nAB-10105\nAdrian Barton\nConsumer\nUnited States\nDetroit\n...\nBinders\nGBC Ibimaster 500 Manual ProClick Binding System\n9892.740\n13\n0.0\n4946.3700\n-0.107894\n-1\n-0.093651\n-1\n\n\n9270\n9271\nUS-2017-102183\n21-08-2017\n28-08-2017\nStandard Class\nPK-19075\nPete Kriz\nConsumer\nUnited States\nNew York City\n...\nBinders\nGBC DocuBind TL300 Electric Binding System\n4305.552\n6\n0.2\n1453.1238\n-0.072519\n-1\n-0.056825\n-1\n\n\n9639\n9640\nCA-2015-116638\n28-01-2015\n31-01-2015\nSecond Class\nJH-15985\nJoseph Holt\nConsumer\nUnited States\nConcord\n...\nTables\nChromcraft Bull-Nose Wood Oval Conference Tabl...\n4297.644\n13\n0.4\n-1862.3124\n-0.072519\n-1\n-0.066153\n-1\n\n\n9741\n9742\nCA-2015-117086\n08-11-2015\n12-11-2015\nStandard Class\nQJ-19255\nQuincy Jones\nCorporate\nUnited States\nBurlington\n...\nBookcases\nRiverside Palais Royal Lawyers Bookcase, Royal...\n4404.900\n5\n0.0\n1013.1270\n-0.074197\n-1\n-0.033171\n-1\n\n\n9929\n9930\nCA-2016-129630\n04-09-2016\n04-09-2016\nSame Day\nIM-15055\nIonia McGrath\nConsumer\nUnited States\nSan Francisco\n...\nCopiers\nCanon PC1060 Personal Laser Copier\n2799.960\n5\n0.2\n944.9865\n-0.015892\n-1\n-0.026842\n-1\n\n\n\n\n57 rows × 25 columns\n\n\n\nMultivariate Anomaly Detection\n\nsns.regplot(x=\"Sales\", y=\"Profit\", data=df)\nsns.despine();\n\n\n\n\n\nminmax = MinMaxScaler(feature_range=(0, 1))\nX = minmax.fit_transform(df[['Sales','Profit']])\n\n\nclf = IsolationForest(n_estimators=100, contamination=0.01, random_state=0)\nclf.fit(X)\n\n# predict raw anomaly score\ndf['multivariate_anomaly_score'] = clf.decision_function(X)\n        \n# prediction of a datapoint category outlier or inlier\ndf['multivariate_outlier'] = clf.predict(X)\n\n\nplt.scatter(df['Sales'], df['Profit'],\n            c=df.multivariate_outlier, edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('Paired', 10))\nplt.xlabel('Sales')\nplt.ylabel('Profit')\nplt.colorbar();\n\n\n\n\n\ndf[(df['outlier_univariate_sales'] == 1) & (df['outlier_univariate_profit'] == 1) & (df['multivariate_outlier'] == -1)]\n\n\n\n\n\n\n\n\nRow ID\nOrder ID\nOrder Date\nShip Date\nShip Mode\nCustomer ID\nCustomer Name\nSegment\nCountry\nCity\n...\nSales\nQuantity\nDiscount\nProfit\nanomaly_score_univariate_sales\noutlier_univariate_sales\nanomaly_score_univariate_profit\noutlier_univariate_profit\nmultivariate_anomaly_score\nmultivariate_outlier\n\n\n\n\n1001\n1002\nCA-2015-124891\n31-07-2015\n31-07-2015\nSame Day\nRH-19495\nRick Hansen\nConsumer\nUnited States\nNew York City\n...\n2309.65\n7\n0.0\n762.1845\n0.005581\n1\n0.002532\n1\n-0.003494\n-1\n\n\n3443\n3444\nCA-2017-168858\n19-11-2017\n23-11-2017\nStandard Class\nJD-16150\nJustin Deggeller\nCorporate\nUnited States\nNew York City\n...\n2504.74\n7\n0.0\n626.1850\n0.000071\n1\n0.029921\n1\n-0.000494\n-1\n\n\n9948\n9949\nCA-2017-121559\n01-06-2017\n03-06-2017\nSecond Class\nHW-14935\nHelen Wasserman\nCorporate\nUnited States\nIndianapolis\n...\n2405.20\n8\n0.0\n793.7160\n0.001090\n1\n0.001520\n1\n-0.009530\n-1\n\n\n\n\n3 rows × 27 columns\n\n\n\n\nminmax = MinMaxScaler(feature_range=(0, 1))\nX = minmax.fit_transform(df[['Sales','Profit']])"
  },
  {
    "objectID": "posts/blog3/index1.html",
    "href": "posts/blog3/index1.html",
    "title": "Blog1",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndataset = pd.read_csv('Mall_Customers.csv')\nX = dataset.iloc[:,[3,4]].values\n\n\n# Using the elbow method to find the optimal number of clusters\n\nfrom sklearn.cluster import KMeans\nwcss =[]\nfor i in range (1,11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter =300, n_init = 10, random_state = 0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\n\n# Plot the graph to visualize the Elbow Method to find the optimal number of cluster  \nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\nkmeans=KMeans(n_clusters= 5, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\nY_Kmeans = kmeans.fit_predict(X)\n\n\n# Visualising the clusters\n\nplt.scatter(X[Y_Kmeans == 0, 0], X[Y_Kmeans == 0,1],s = 100, c='red', label = 'Cluster 1')\n\nplt.scatter(X[Y_Kmeans == 1, 0], X[Y_Kmeans == 1,1],s = 100, c='blue', label = 'Cluster 2')\n\nplt.scatter(X[Y_Kmeans == 2, 0], X[Y_Kmeans == 2,1],s = 100, c='green', label = 'Cluster 3')\n\nplt.scatter(X[Y_Kmeans == 3, 0], X[Y_Kmeans == 3,1],s = 100, c='cyan', label = 'Cluster 4')\n\nplt.scatter(X[Y_Kmeans == 4, 0], X[Y_Kmeans == 4,1],s = 100, c='magenta', label = 'Cluster 5')\n\nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s = 300, c = 'yellow', label = 'Centroids')\n    \nplt.title('Clusters of clients')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending score (1-100)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Linear Regression and Non Linear Regression",
    "section": "",
    "text": "# Import Libary\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Download and prepare the data\ndf=pd.read_csv(\"headbrain.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nGender\nAge Range\nHead Size(cm^3)\nBrain Weight(grams)\n\n\n\n\n0\n1\n1\n4512\n1530\n\n\n1\n1\n1\n3738\n1297\n\n\n2\n1\n1\n4261\n1335\n\n\n3\n1\n1\n3777\n1282\n\n\n4\n1\n1\n4177\n1590\n\n\n\n\n\n\n\n\n# Import Libary\nprint(df.isnull().sum())\n\n    \n# Declare dependent variable(Y) and independent variable(X)\n\nX=df['Head Size(cm^3)'].values\nY = df['Brain Weight(grams)'].values\n\nGender                 0\nAge Range              0\nHead Size(cm^3)        0\nBrain Weight(grams)    0\ndtype: int64\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 237 entries, 0 to 236\nData columns (total 4 columns):\n #   Column               Non-Null Count  Dtype\n---  ------               --------------  -----\n 0   Gender               237 non-null    int64\n 1   Age Range            237 non-null    int64\n 2   Head Size(cm^3)      237 non-null    int64\n 3   Brain Weight(grams)  237 non-null    int64\ndtypes: int64(4)\nmemory usage: 7.5 KB\n\n\n\ndf.shape\n\n(237, 4)\n\n\n\nX.shape\n\n(237,)\n\n\n\nY.shape\n\n(237,)\n\n\n\nnp.corrcoef(X, Y)\n\narray([[1.        , 0.79956971],\n       [0.79956971, 1.        ]])\n\n\n\n# Plot the Input Data\nplt.scatter(X, Y, c='green', label='Data points')\nplt.xlabel('Head Size in cm3')\nplt.ylabel('Brain Weight in grams')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Plot the Input Data\n# Calculating coefficient\n\n# Mean X and Y\nmean_x = np.mean(X)\nmean_y = np.mean(Y)\n\n# Total number of values\nn = len(X)\n\n# Using the formula to calculate theta1 and theta2\nnumer = 0\ndenom = 0\nfor i in range(n):\n    numer += (X[i] - mean_x) * (Y[i] - mean_y)\n    denom += (X[i] - mean_x) ** 2\nb1 = numer / denom\nb0 = mean_y - (b1 * mean_x)\n\n# Printing coefficients\nprint(\"coefficients for regression\",b1, b0)\n\ncoefficients for regression 0.26342933948939945 325.57342104944223\n\n\n\n# Plotting Values and Regression Line\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (10.0, 5.0)\n# max_x = np.max(X) + 100\n# min_x = np.min(X) - 100\n\ny = b0 + b1 * X\n\n# Ploting Line\nplt.plot(X, y, color='blue', label='Regression Line')\n# Ploting Scatter Points\nplt.scatter(X, Y, c='green', label='Scatter data')\n\nplt.xlabel('Head Size in cm3')\nplt.ylabel('Brain Weight in grams')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Calculating Root Mean Squares Error\nrmse = 0\nfor i in range(n):\n    y_pred = b0 + b1 * X[i]\n    rmse += (Y[i] - y_pred) ** 2\n    \nrmse = np.sqrt(rmse/n)\nprint(\"Root Mean Square Error is\",rmse)\n\nRoot Mean Square Error is 72.1206213783709\n\n\n\n# Calculating R2 Score\nss_tot = 0\nss_res = 0\nfor i in range(n):\n    y_pred = b0 + b1 * X[i]\n    ss_tot += (Y[i] - mean_y) ** 2\n    ss_res += (Y[i] - y_pred) ** 2\nr2 = 1 - (ss_res/ss_tot)\nprint(\"R2 Score\",r2)\n\nR2 Score 0.6393117199570003\n\n\n##Method 2:\n\n# Import necessary libraries\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('headbrain.csv')\n\n# Explore the dataset\ndf.head()\n\n\n\n\n\n\n\n\nGender\nAge Range\nHead Size(cm^3)\nBrain Weight(grams)\n\n\n\n\n0\n1\n1\n4512\n1530\n\n\n1\n1\n1\n3738\n1297\n\n\n2\n1\n1\n4261\n1335\n\n\n3\n1\n1\n3777\n1282\n\n\n4\n1\n1\n4177\n1590\n\n\n\n\n\n\n\n\nprint(df.isnull().sum())\n\nGender                 0\nAge Range              0\nHead Size(cm^3)        0\nBrain Weight(grams)    0\ndtype: int64\n\n\n\nmean_x = np.mean(X)\nmean_y = np.mean(Y)\n\n#Total number of Values\nn = len(X)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX = X.reshape((n,1))\n\n\nmodel = LinearRegression()\n\n\nmodel = model.fit(X,Y)\n\n\nr2 = model.score(X,Y)\nprint('R^2 value: ',r2)\n\nR^2 value:  0.639311719957\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = df[['Head Size(cm^3)']]  # Select relevant features\ny = df['Brain Weight(grams)']  # Define the target variable\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmodel = LinearRegression() # Create a linear regression model\n\nmodel.fit(X_train, y_train) # Fit the model to the training data\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred = model.predict(X_test)\n\n\nplt.scatter(y_test, y_pred) # Scatter plot to visualize actual vs. predicted values\n\nplt.xlabel(\"Actual Sale Prices\")\nplt.ylabel(\"Predicted Sale Prices\")\nplt.title(\"Actual Sale Prices vs. Predicted Sale Prices\")\nplt.show()\n\n\n\n\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate R-squared (R²) score\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error (MSE): {mse:.2f}\")\nprint(f\"R-squared (R²) Score: {r2:.2f}\")\n\nMean Squared Error (MSE): 4672.04\nR-squared (R²) Score: 0.71\n\n\n#Non Linear Regression\n##Method 1 (Manual dataset)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#define predictor and response variables\nx = np.array([2, 3, 4, 5, 6, 7, 7, 8, 9, 11, 12])\ny = np.array([18, 16, 15, 17, 20, 23, 25, 28, 31, 30, 29])\n\n#create scatterplot to visualize relationship between x and y\nplt.scatter(x, y)\n\n&lt;matplotlib.collections.PathCollection at 0x1dd70cab640&gt;\n\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n#specify degree of 3 for polynomial regression model\n#include bias=False means don't force y-intercept to equal zero\npoly = PolynomialFeatures(degree=3, include_bias=False)\n\n#reshape data to work properly with sklearn\npoly_features = poly.fit_transform(x.reshape(-1, 1))\n\n#fit polynomial regression model\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(poly_features, y)\n\n#display model coefficients\nprint(poly_reg_model.intercept_, poly_reg_model.coef_)\n\n33.62640037532293 [-11.83877127   2.25592957  -0.10889554]\n\n\n\n#use model to make predictions on response variable\ny_predicted = poly_reg_model.predict(poly_features)\n\n#create scatterplot of x vs. y\nplt.scatter(x, y)\n\n#add line to show fitted polynomial regression model\nplt.plot(x, y_predicted, color='purple')\n\n\n\n\n##Method 2\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n#from sklearn.linear_model import LinearRegression\n\n# Download and prepare the data\ndf=pd.read_csv(\"headbrain.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nGender\nAge Range\nHead Size(cm^3)\nBrain Weight(grams)\n\n\n\n\n0\n1\n1\n4512\n1530\n\n\n1\n1\n1\n3738\n1297\n\n\n2\n1\n1\n4261\n1335\n\n\n3\n1\n1\n3777\n1282\n\n\n4\n1\n1\n4177\n1590\n\n\n\n\n\n\n\n\nX=df['Head Size(cm^3)'].values\nY = df['Brain Weight(grams)'].values\n\n\n# Fitting Polynomial Regression to the dataset\nfrom sklearn.preprocessing import PolynomialFeatures\nX=X.reshape(-1,1)\nY=Y.reshape(-1,1)\npoly = PolynomialFeatures(degree=4)\nX_poly = poly.fit_transform(X)\n \npoly.fit(X_poly, Y)\nlin2 = LinearRegression()\n\n\nlin2.fit(X_poly, Y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Visualising the Polynomial Regression results\nplt.scatter(X, Y, color='blue')\n \nplt.plot(X, lin2.predict(poly.fit_transform(X)),\n         color='red')\nplt.title('Polynomial Regression')\nplt.xlabel('Head')\nplt.ylabel('Brain')\n \nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog1",
    "section": "",
    "text": "Using the elbow method to find the optimal number of clusters\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\npredict raw anomaly score\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression and Non Linear Regression\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nZeel\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nZeel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n# Import Libary\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# Libraries for visualization:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndata=pd.read_csv(\"breast-cancer.csv\")\n\n\ndata.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n# Download and prepare the data\ndata.drop([\"id\"],axis=1,inplace=True)\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\ncount\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n0.062798\n...\n16.269190\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n\n\nstd\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n0.007060\n...\n4.833242\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n\n\nmin\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n0.049960\n...\n7.930000\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n\n\n25%\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n0.057700\n...\n13.010000\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n\n\n50%\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n0.061540\n...\n14.970000\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n\n\n75%\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n0.066120\n...\n18.790000\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n\n\nmax\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n0.097440\n...\n36.040000\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n\n\n\n\n8 rows × 30 columns\n\n\n\n\n# Check for missing value:\ndata.isnull().sum()\n\ndiagnosis                  0\nradius_mean                0\ntexture_mean               0\nperimeter_mean             0\narea_mean                  0\nsmoothness_mean            0\ncompactness_mean           0\nconcavity_mean             0\nconcave points_mean        0\nsymmetry_mean              0\nfractal_dimension_mean     0\nradius_se                  0\ntexture_se                 0\nperimeter_se               0\narea_se                    0\nsmoothness_se              0\ncompactness_se             0\nconcavity_se               0\nconcave points_se          0\nsymmetry_se                0\nfractal_dimension_se       0\nradius_worst               0\ntexture_worst              0\nperimeter_worst            0\narea_worst                 0\nsmoothness_worst           0\ncompactness_worst          0\nconcavity_worst            0\nconcave points_worst       0\nsymmetry_worst             0\nfractal_dimension_worst    0\ndtype: int64\n\n\n\ndata.hist(figsize = (18,17),color = 'orange',edgecolor = 'black');\n\n\n\n\nData Visualization\nHistogram A histogram is a bar graph representation of a grouped data distribution. In other words, it is the transfer of data consisting of repetitive numbers to the table first, and to the chart by using the table, in other words, the graph of the data groups is displayed in rectangular columns.\n\n# Check strength of the relationship between variables:\ncorr=data.corr(numeric_only = True)\nf,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(corr,annot=True,linewidths=0.5,fmt=\".1f\",ax=ax,cmap=\"YlGnBu\",square=True)\nplt.show()\n\n\n\n\nCountplot and PiePlot A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. A Pie Chart is a type of graph that displays data in a circular graph. The pieces of the graph are proportional to the fraction of the whole in each category.\nWe examined distribution of outcome with countplot and pieplot.\n\nprint('Count of M or B cells in diagnosis:')\ndata['diagnosis'].value_counts()\n\nCount of M or B cells in diagnosis:\n\n\ndiagnosis\nB    357\nM    212\nName: count, dtype: int64\n\n\n\n# Plot distribution\ndata['diagnosis'].value_counts().plot(kind='pie', labels = ['', ''], autopct = '%1.1F%%', colors = ['#9091b1','#e1b0cd'], \n                                    explode = [0,0.05], textprops = {'fontsize':15})\nplt.legend(labels=['Benign', 'Malignant'], fontsize=12)\nplt.title('Distributions of the target variable\\n', fontsize=20, color = '#6a6a6a', y=1.03)\nplt.show()\n\n\n\n\n\nsns.countplot(x='diagnosis',data=data,palette='Greens_d')\nplt.show()\n\n## M for Malignant\n## B for Benign\n\n\n\n\n\n#Libraries for ML model\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,recall_score,precision_score,roc_auc_score\n\n\ny=data[\"diagnosis\"]\nX=data.drop([\"diagnosis\"],axis=1)\nprint(\"X shape\",X.shape)\nprint(\"y shape\",y.shape)\n\nX shape (569, 30)\ny shape (569,)\n\n\n\none_hot=LabelBinarizer()\ny=one_hot.fit_transform(y)\n\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42,shuffle=True)\n\n\nprint(\"X_train shape\",X_train.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"X_test shape\",X_test.shape)\nprint(\"y_test shape\",y_test.shape)\n\nX_train shape (398, 30)\ny_train shape (398, 1)\nX_test shape (171, 30)\ny_test shape (171, 1)\n\n\n\n# Lets create model:\ndef classification_models(model):\n    y_pred=model.fit(X_train,y_train).predict(X_test)\n    accuracy=accuracy_score(y_pred,y_test)\n    roc_score=roc_auc_score(y_test,y_pred)\n    f1=f1_score(y_pred,y_test)\n    precision=precision_score(y_pred,y_test)\n    recall=recall_score(y_pred,y_test)\n    \n    results=pd.DataFrame({\"Values\":[accuracy,roc_score,f1,precision,recall],\n                         \"Metrics\":[\"Accuracy\",\"ROC-AUC\",\"F1\",\"Precision\",\"Recall\"]})\n    \n    # Visualize Results:\n    fig=make_subplots(rows=1,cols=1)\n    fig.add_bar(x=[round(i,5) for i in results[\"Values\"]],\n                        y=results[\"Metrics\"],\n                        text=[round(i,5) for i in results[\"Values\"]],orientation=\"h\",textposition=\"inside\",name=\"Values\",\n                        marker=dict(color=[\"khaki\",\"bisque\",\"palegreen\",\"skyblue\",\"plum\"],line_color=\"beige\",line_width=1.5),row=1,col=1)\n    fig.update_layout(title={'text': model.__class__.__name__ ,\n                             'y':0.9,\n                             'x':0.5,\n                             'xanchor': 'center',\n                             'yanchor': 'top'},\n                      template='plotly_white')\n    fig.update_xaxes(range=[0,1], row = 1, col = 1)\n    \n    iplot(fig)\n    \nmy_models= [\n    \n    LogisticRegression(),\n    KNeighborsClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    DecisionTreeClassifier()\n\n\n]\n\nfor model in my_models:\n    classification_models(model)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\nprint(\"X_train shape\",X_train.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"X_test shape\",X_test.shape)\nprint(\"y_test shape\",y_test.shape)\n\nX_train shape (398, 30)\ny_train shape (398, 1)\nX_test shape (171, 30)\ny_test shape (171, 1)"
  },
  {
    "objectID": "posts/blog4/index.html",
    "href": "posts/blog4/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndatasets = pd.read_csv('SocialNetworks.csv')\nX = datasets.iloc[:, [2,3]].values\nY = datasets.iloc[:, 4].values\n\n\n# Splitting the dataset into the Training set and Test set\n\nfrom sklearn.model_selection import train_test_split\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n\n\n# Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_Train = sc_X.fit_transform(X_Train)\nX_Test = sc_X.transform(X_Test)\n\n\n# Fitting the Logistic Regression into the Training set\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_Train, Y_Train)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\n\n# Visualising the clusters\n\n# Predicting the test set results\n\nY_Pred = classifier.predict(X_Test)\n\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_Test, Y_Pred)\n\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_Test, Y_Pred)\n\n\ncm\n\narray([[65,  3],\n       [ 8, 24]], dtype=int64)\n\n\n\n# Visualising the Training set results \n\nfrom matplotlib.colors import ListedColormap\nX_Set, Y_Set = X_Train, Y_Train\nX1, X2 = np.meshgrid(np.arange(start = X_Set[:,0].min() -1, stop = X_Set[:, 0].max() +1, step = 0.01),\n                     np.arange(start = X_Set[:,1].min() -1, stop = X_Set[:, 1].max() +1, step = 0.01))\n\nplt.contourf(X1,X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n\nplt.xlim(X1.min(), X2.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_Set)):\n    plt.scatter(X_Set[Y_Set == j, 0], X_Set[Y_Set == j,1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression ( Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\nC:\\Users\\desai\\AppData\\Local\\Temp\\ipykernel_11936\\3356985325.py:14: UserWarning:\n\n*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points."
  }
]